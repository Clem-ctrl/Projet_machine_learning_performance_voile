import requests
import xml.etree.ElementTree as ET
import pandas as pd
import re
import time
import os
import glob
import random
import sys
import shutil
import tempfile
import json
from urllib.parse import urlparse, parse_qs
from datetime import datetime
from bs4 import BeautifulSoup

# Importations spÃ©cifiques Ã  Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import WebDriverException, TimeoutException

# --- Constantes pour un scraping plus "poli" et plus robuste ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
]
MIN_DELAY_SECONDS = 9
MAX_DELAY_SECONDS = 30
MAX_RETRIES = 5

# --- Configuration du rÃ©pertoire de profil persistant pour Selenium ---
PERSISTENT_DATA_DIR = os.path.join(os.path.expanduser('~'), 'selenium_chrome_profile')
if not os.path.exists(PERSISTENT_DATA_DIR):
    os.makedirs(PERSISTENT_DATA_DIR)
    print(f"CrÃ©ation du rÃ©pertoire de profil persistant : {PERSISTENT_DATA_DIR}")


# -----------------------------------------------------------
# Fonction pour gÃ©rer l'extraction d'URLs des fichiers locaux

def find_urls_from_local_files(directory_path):
    """
    Recherche les fichiers HTML locaux et extrait les URLs Metasail qui y sont contenues.
    """
    print("Ã‰tape 1 : Recherche des URLs dans les fichiers locaux... ğŸ”")
    search_pattern = os.path.join(directory_path, "MetaSail for web*.html")
    html_files = glob.glob(search_pattern)
    urls = []
    url_regex = re.compile(r'https://app\.metasail\.it/ViewRecordedRace2018\.aspx\?idgara=\d+&amp;token=\w+')

    if not html_files:
        print(f"    -> âŒ Aucun fichier correspondant Ã  '{search_pattern}' n'a Ã©tÃ© trouvÃ©.")
        return []

    print(f"    -> âœ… Fichiers trouvÃ©s : {len(html_files)}")
    for file_path in html_files:
        filename = os.path.basename(file_path)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                found_urls = [url.replace('&amp;', '&') for url in url_regex.findall(content)]
                if found_urls:
                    urls.extend(found_urls)
                    print(f"      -> âœ… TrouvÃ© {len(found_urls)} URL(s) dans '{filename}'")
        except Exception as e:
            print(f"      -> âŒ Erreur lors de la lecture du fichier {filename}: {e}", file=sys.stderr)

    unique_urls = sorted(list(set(urls)))
    if unique_urls:
        print(f"    -> âœ… Total d'URLs uniques trouvÃ©es : {len(unique_urls)}")
    else:
        print("    -> âŒ Aucune URL de course valide n'a Ã©tÃ© trouvÃ©e dans les fichiers locaux.")

    return unique_urls


# -----------------------------------------------------------
class MetasailScraper:
    """Classe pour encapsuler la logique de scraping d'une URL Metasail."""

    def __init__(self, event_url, event_id, token, source_name, session):
        self.event_url = event_url
        self.stats_url = None
        self.event_id = event_id
        self.token = token
        self.source_name = source_name
        self.session = session
        self.event_name, self.race_name, self.event_location, self.race_date, self.stats_data = None, "N/A", "N/A", "N/A", None
        self.wind_orientation_metasail = None
        self.translations = {
            'Seriale': 'NumÃ©ro de sÃ©rie', 'Nome': 'Nom complet',
            'TotTempPerc': 'Temps total parcouru (s)', 'TotLungLato': 'Longueur totale du parcours (m)',
            'TotDistPerc': 'Distance totale rÃ©elle parcourue (m)', 'PosPartenza': 'Position de dÃ©part',
            'TotDistRealeSuIdeale': 'EfficacitÃ© (Distance rÃ©elle/idÃ©ale) (%)', 'SegNum': 'NumÃ©ro de segment',
            'TopSpeed': 'Vitesse maximale (noeuds)', 'TopVMG': 'VMG maximale', 'TopVMC': 'VMC maximale',
            'AvgVMG': 'VMG moyenne', 'AvgVMC': 'VMC moyenne', 'AvgSpeed': 'Vitesse moyenne (noeuds)',
            'CrtRaceSegSX': 'BÃ¢bord (%)', 'CrtRaceSegDX': 'Tribord (%)',
            'TimeSecPercorsi': 'Temps du segment (s)', 'SegDistRealePercorsa': 'Distance rÃ©elle parcourue segment (m)',
            'LungLato': 'Longueur du segment (m)', 'DirLato': 'Cap magnÃ©tique (deg)',
            'PercEffettivo': 'EfficacitÃ© du segment (%)', 'StartSeg': 'DÃ©but du segment (timestamp)',
            'EndSeg': 'Fin du segment (timestamp)', 'SegEnteredRank': 'Classement entrÃ©e de segment',
            'SegExitRank': 'Classement sortie de segment',
        }

    def get_page_info_with_selenium(self, driver):
        """
        RÃ©cupÃ¨re les informations de la page en utilisant Selenium.
        Le script fait glisser le curseur du temps pour charger les donnÃ©es, avec une logique de rÃ©essai.
        Le driver est passÃ© en argument pour rÃ©utiliser la mÃªme session.
        """
        try:
            print(f"Ã‰tape 2 : RÃ©cupÃ©ration des infos de la course... â›µ")
            print(f"    -> ğŸŒ Navigation vers : {self.event_url}")
            driver.get(self.event_url)

            wait = WebDriverWait(driver, 45)

            print("    -> â³ Attente de l'apparition du curseur de la barre de temps...")
            slider_handle = wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, 'rangeSlider__handle'))
            )

            # --- Nouvelle fonctionnalitÃ© : attente pour un dÃ©placement plus doux ---
            print("    -> â³ Attente de 3 secondes avant le premier dÃ©placement du curseur...")
            time.sleep(3)

            wind_data_found = False
            for attempt_move in range(MAX_RETRIES):
                if wind_data_found:
                    break

                # VÃ©rification initiale si les donnÃ©es de vent sont dÃ©jÃ  chargÃ©es
                try:
                    wind_text = driver.find_element(By.ID, 'lblWind').text
                    if wind_text and wind_text.strip():
                        print(f"    -> âœ… DonnÃ©es de vent dÃ©jÃ  prÃ©sentes. Pas besoin de dÃ©placer le curseur.")
                        wind_data_found = True
                        break
                except:
                    pass

                # --- Mise Ã  jour de la logique de dÃ©placement pour une fiabilitÃ© accrue ---
                print(f"    -> ğŸ”„ Tentative de glisser-dÃ©poser du curseur {attempt_move + 1}/{MAX_RETRIES}...")

                try:
                    actions = ActionChains(driver)
                    # La mÃ©thode drag_and_drop_by_offset est plus stable car elle opÃ¨re directement sur l'Ã©lÃ©ment.
                    # On simule un glisser-dÃ©poser du curseur sur 100 pixels.
                    actions.drag_and_drop_by_offset(slider_handle, 100, 0).perform()
                except Exception as e:
                    print(f"    -> âŒ Le glisser-dÃ©poser a Ã©chouÃ© : {e}. RÃ©essai...")

                # --- Attente pour laisser le temps Ã  la page de charger les donnÃ©es ---
                print("    -> â³ Attente de 2 secondes pour le chargement des donnÃ©es...")
                time.sleep(2)

                try:
                    wind_text = driver.find_element(By.ID, 'lblWind').text
                    if wind_text and wind_text.strip():
                        print(f"    -> âœ… DonnÃ©es de vent trouvÃ©es aprÃ¨s un glisser-dÃ©poser rÃ©ussi.")
                        wind_data_found = True
                        break
                except:
                    print("    -> âŒ DonnÃ©es de vent non trouvÃ©es. RÃ©essai...")

            if not wind_data_found:
                print(
                    "    -> âŒ Le curseur n'a pas pu Ãªtre dÃ©placÃ© ou les donnÃ©es de vent n'ont jamais Ã©tÃ© trouvÃ©es aprÃ¨s plusieurs tentatives.")
                raise TimeoutException("Ã‰chec du dÃ©placement du curseur.")

            wind_direction_text = driver.find_element(By.ID, 'lblWind').text
            wind_direction_text = re.sub(r'Wind|Â°', '', wind_direction_text).strip()

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            script_content = next((s.text for s in soup.find_all('script') if 'dataLayer.push' in s.text), "")

            event_name, race_name, race_date = "N/A", "N/A", "N/A"
            if script_content:
                try:
                    data_layer_match = re.search(r'dataLayer\.push\(({.*?})\);', script_content, re.DOTALL)
                    if data_layer_match:
                        json_str = data_layer_match.group(1).replace("'", '"')
                        data_dict = json.loads(json_str)
                        event_name = data_dict.get('eventName', "N/A")
                        race_name = data_dict.get('race', "N/A")
                        race_date = data_dict.get('raceDate', "N/A")
                        print("    -> âœ… DonnÃ©es 'dataLayer' trouvÃ©es et extraites avec succÃ¨s.")
                except Exception as e:
                    print(f"    -> âŒ Erreur lors de l'analyse du JSON 'dataLayer': {e}")

            final_url = driver.current_url
            cookies_list = driver.get_cookies()
            for cookie in cookies_list:
                self.session.cookies.set(cookie['name'], cookie['value'])
            print("    -> âœ… Cookies de Selenium ajoutÃ©s Ã  la session requests.")
            print(f"    -> âœ… Page rÃ©cupÃ©rÃ©e avec succÃ¨s. URL finale : {final_url}")
            print(f"    -> âœ… Orientation du vent trouvÃ©e : {wind_direction_text}")
            print(f"    -> âœ… Nom de la compÃ©tition : {event_name}")
            print(f"    -> âœ… Nom de la course : {race_name}")
            print(f"    -> âœ… Date de la course : {race_date}")

            return final_url, wind_direction_text, event_name, race_name, race_date

        except (WebDriverException, TimeoutException) as e:
            print(f"    -> âŒ Erreur lors de l'utilisation de Selenium : {e}")

        return None, None, None, None, None

    def _make_request(self, method, url, **kwargs):
        """Effectue une requÃªte HTTP avec dÃ©lais, User-Agent alÃ©atoire et tentatives multiples."""
        print(f"\nÃ‰tape 3 : RÃ©cupÃ©ration des donnÃ©es statistiques... ğŸ“Š")
        for attempt in range(MAX_RETRIES):
            try:
                delay = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)
                print(
                    f"    -> â³ (Tentative {attempt + 1}/{MAX_RETRIES}) Pause de {delay:.2f}s avant la requÃªte...")
                time.sleep(delay)
                headers = {
                    "User-Agent": random.choice(USER_AGENTS),
                    "Accept": "*/*",
                    "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
                    "Referer": "https://app.metasail.it/"
                }
                if 'headers' in kwargs:
                    headers.update(kwargs['headers'])
                kwargs['headers'] = headers
                response = self.session.request(method, url, timeout=30, **kwargs)
                response.raise_for_status()
                print(f"    -> âœ… RequÃªte rÃ©ussie (code {response.status_code}).")
                return response
            except requests.exceptions.RequestException as e:
                print(f"    -> âŒ Erreur de requÃªte (tentative {attempt + 1}): {e}", file=sys.stderr)
                if attempt + 1 == MAX_RETRIES:
                    print("    -> âŒ Nombre maximal de tentatives atteint. Abandon.")
                    return None
        return None

    def _get_stats_data(self):
        """RÃ©cupÃ¨re les donnÃ©es statistiques via une requÃªte POST."""
        if not self.stats_url:
            print("    -> âŒ Erreur: L'URL des statistiques n'a pas Ã©tÃ© construite.")
            return False

        headers = {'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'}
        params = {'idgara': self.event_id, 'token': self.token}
        response = self._make_request('POST', self.stats_url, headers=headers, data=params)

        if response:
            print("    -> âœ… DonnÃ©es brutes reÃ§ues. Nettoyage du XML...")
            xml_content = response.text
            xml_content = re.sub(r'<\?xml.*?\?>', '', xml_content).strip()

            # --- Correction de l'avertissement de dÃ©prÃ©ciation ---
            # Le paramÃ¨tre 'count' est maintenant explicitement nommÃ© pour Ã©viter la dÃ©prÃ©ciation
            xml_content = re.sub(r'<string.*?>', '', xml_content, count=1).rsplit('</string>', 1)[0]
            self.stats_data = xml_content
            print("    -> âœ… DonnÃ©es statistiques prÃªtes Ã  Ãªtre analysÃ©es.")
            return True
        else:
            print("    -> âŒ Ã‰chec de la rÃ©cupÃ©ration des donnÃ©es statistiques aprÃ¨s plusieurs tentatives.")
            return False

    @staticmethod
    def _extract_time_from_timestamp(timestamp):
        try:
            return datetime.fromtimestamp(float(timestamp)).strftime("%H:%M:%S")
        except (ValueError, TypeError, OSError):
            return None

    def _parse_and_prepare_dataframe(self):
        print("Ã‰tape 4 : Analyse du XML et crÃ©ation du DataFrame... ğŸ—ƒï¸")
        if not self.stats_data or not self.event_name:
            print("    -> âŒ DonnÃ©es ou nom d'Ã©vÃ©nement manquants. Ã‰chec de l'analyse.")
            return None
        try:
            root = ET.fromstring(self.stats_data)
            data_rows = []
            namespace = '{http://meteda.it/}'
            print("    -> ğŸ” DÃ©but de l'extraction des donnÃ©es des coureurs et segments.")
            for racer_data in root.findall(f'.//{namespace}StatisticheDato'):
                racer_info = {'ID': self.source_name, 'Nom de l\'Ã©vÃ©nement': self.event_name,
                              'Lieu de l\'Ã©vÃ©nement': self.event_location, 'Course': self.race_name,
                              'Date de la course': self.race_date,
                              'Orientation vent metasail': self.wind_orientation_metasail}

                full_name_element = racer_data.find(f'.//{namespace}Nome')
                racer_info['Nom complet'] = full_name_element.text if full_name_element is not None else "N/A"

                print(f"      -> Traitement du coureur : {racer_info['Nom complet']}")

                for child in racer_data:
                    tag_name = child.tag.replace(namespace, '')
                    if tag_name in self.translations and tag_name != 'Nome' and child.text:
                        racer_info[self.translations[tag_name]] = child.text

                segments = racer_data.findall(f'.//{namespace}cInfoRaceSegment')
                if not segments:
                    data_rows.append(racer_info)
                    print("        -> âš ï¸ Aucun segment trouvÃ© pour ce coureur. DonnÃ©es gÃ©nÃ©rales ajoutÃ©es.")
                else:
                    for i, segment_data in enumerate(segments):
                        segment_row = racer_info.copy()
                        for child in segment_data:
                            tag_name = child.tag.replace(namespace, '')
                            if tag_name in self.translations and child.text:
                                segment_row[self.translations[tag_name]] = self._extract_time_from_timestamp(
                                    child.text) if tag_name in ['StartSeg', 'EndSeg'] else child.text
                        data_rows.append(segment_row)
                        print(f"        -> âœ… DonnÃ©es du segment {i + 1} ajoutÃ©es.")

            if not data_rows:
                print("    -> âŒ Aucune donnÃ©e Ã  transformer en DataFrame.")
                return None
            df = pd.DataFrame(data_rows)
            first_cols = ['ID', 'Nom de l\'Ã©vÃ©nement', 'Lieu de l\'Ã©vÃ©nement', 'Course', 'Date de la course',
                          'Orientation vent metasail', 'Nom complet', self.translations['Seriale']]
            other_cols = [col for col in df.columns if col not in first_cols]
            df = df.reindex(columns=first_cols + other_cols)
            print("    -> âœ… DataFrame crÃ©Ã© avec succÃ¨s.")
            return df
        except ET.ParseError as e:
            print(f"    -> âŒ Erreur XML: {e}", file=sys.stderr)
            return None

    def scrape_and_export(self, driver, output_filename):
        final_url, wind_direction, event_name, race_name, race_date = self.get_page_info_with_selenium(driver)

        if not final_url:
            print("    -> ğŸ›‘ ArrÃªt du processus pour cette URL en raison d'un Ã©chec de la rÃ©cupÃ©ration des infos.")
            return False

        self.event_url = final_url
        self.wind_orientation_metasail = wind_direction
        self.event_name = event_name
        self.race_name = race_name
        self.race_date = race_date

        parsed_url = urlparse(self.event_url)
        session_prefix_match = re.search(r'\(S\((.*?)\)\)', self.event_url)
        session_prefix = f"(S({session_prefix_match.group(1)}))" if session_prefix_match else ""
        self.stats_url = f"https://app.metasail.it/{session_prefix}/MetaSailWS.asmx/getStatistiche?idgara={self.event_id}&token={self.token}"
        print(f"    -> âœ… URL des statistiques construite : {self.stats_url}")

        if not self._get_stats_data():
            print("    -> ğŸ›‘ ArrÃªt du processus pour cette URL en raison d'un Ã©chec de la rÃ©cupÃ©ration des donnÃ©es.")
            return False

        df_new = self._parse_and_prepare_dataframe()
        if df_new is not None and not df_new.empty:
            print("\nÃ‰tape 5 : Exportation des donnÃ©es vers Excel... ğŸ’¾")
            try:
                if os.path.exists(output_filename):
                    print(f"    -> ğŸ’¾ Le fichier '{output_filename}' existe. Ajout des nouvelles donnÃ©es.")
                    with pd.ExcelWriter(output_filename, engine='openpyxl', mode='a',
                                        if_sheet_exists='overlay') as writer:
                        df_existing = pd.read_excel(output_filename, sheet_name='Sheet1')
                        df_final = pd.concat([df_existing, df_new], ignore_index=True)
                        df_final.to_excel(writer, index=False, sheet_name='Sheet1')
                        print("    -> âœ… DonnÃ©es ajoutÃ©es avec succÃ¨s.")
                else:
                    print(f"    -> ğŸ’¾ Le fichier '{output_filename}' n'existe pas. CrÃ©ation...")
                    df_new.to_excel(output_filename, index=False)
                    print("    -> âœ… Fichier crÃ©Ã© et donnÃ©es exportÃ©es avec succÃ¨s.")
            except Exception as e:
                print(f"    -> âŒ Erreur lors de l'exportation Excel: {e}", file=sys.stderr)
            return True
        else:
            print("    -> âŒ Ã‰chec de l'exportation : Le DataFrame est vide.")
            return False


# ====================================================================
# --- LOGIQUE PRINCIPALE DU SCRIPT ---
# ====================================================================

if __name__ == "__main__":
    print("ğŸš€ DÃ©but du script de scraping Metasail.")

    # --- âš ï¸ REMPLACEZ LES CHEMINS CI-DESSOUS âš ï¸ ---
    LOCAL_DIRECTORY_PATH = r"C:\Users\Byron Barette\Downloads"
    OUTPUT_FILENAME = "Metasail_Statistics_unified.xlsx"

    urls_to_process = []
    processed_files = set()
    successful_urls = 0
    failed_urls = 0

    # --- Initialisation du driver une seule fois ---
    try:
        options = Options()
        # options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
        options.add_argument(f'--user-data-dir={PERSISTENT_DATA_DIR}')
        driver = webdriver.Chrome(options=options)

        with requests.Session() as session:
            urls_to_process = find_urls_from_local_files(LOCAL_DIRECTORY_PATH)

            if not urls_to_process:
                print("    -> âŒ Aucune URL de course trouvÃ©e. Fin du script.")
                sys.exit()

            print("\nÃ‰tape 6 : VÃ©rification des doublons dans le fichier de sortie... ğŸ•µï¸â€â™€ï¸")
            if os.path.exists(OUTPUT_FILENAME):
                try:
                    print(f"    -> ğŸ” Lecture du fichier '{OUTPUT_FILENAME}'...")
                    df_existing = pd.read_excel(OUTPUT_FILENAME, sheet_name='Sheet1')
                    if 'ID' in df_existing.columns:
                        processed_files = set(df_existing['ID'].dropna().unique())
                        print(f"    -> âœ… {len(processed_files)} fichier(s) dÃ©jÃ  traitÃ©(s) trouvÃ©(s).")
                    else:
                        print(
                            "    -> âš ï¸ Avertissement : La colonne 'ID' est absente. Impossible de vÃ©rifier les doublons.")
                except Exception as e:
                    print(f"    -> âŒ Avertissement : Impossible de lire le fichier existant. {e}")

            total_urls = len(urls_to_process)
            for i, source_url in enumerate(urls_to_process):
                source_name = urlparse(source_url).query
                print(f"\n--- ğŸ Traitement de l'URL {i + 1}/{total_urls} : {source_url} ---")
                if source_name in processed_files:
                    print(f"    -> â­ï¸ L'URL '{source_name}' a dÃ©jÃ  Ã©tÃ© traitÃ©e. Passage Ã  la suivante.")
                    successful_urls += 1
                    continue
                try:
                    parsed_url = urlparse(source_url)
                    query_params = parse_qs(parsed_url.query)
                    event_id, token = query_params.get('idgara', [None])[0], query_params.get('token', [None])[0]
                    if not event_id or not token:
                        print(f"    -> âŒ ERREUR: idgara/token introuvable dans : {source_url}.")
                        failed_urls += 1
                        continue
                    scraper = MetasailScraper(
                        event_url=source_url,
                        event_id=event_id,
                        token=token,
                        source_name=source_name,
                        session=session
                    )
                    if scraper.scrape_and_export(driver=driver, output_filename=OUTPUT_FILENAME):
                        successful_urls += 1
                    else:
                        failed_urls += 1
                except Exception as e:
                    print(f"    -> âŒ Une erreur inattendue est survenue pour l'URL {source_url}: {e}", file=sys.stderr)
                    failed_urls += 1

    finally:
        if 'driver' in locals() and driver:
            print("\n--- ğŸŒ Fermeture de la session Chrome... ---")
            driver.quit()

    print("\n" + "=" * 50)
    print("--- âœ… Rapport final de scraping âœ… ---")
    print("=" * 50)
    print(f"Total d'URLs traitÃ©es : {total_urls}")
    print(f"URLs avec succÃ¨s : {successful_urls} âœ…")
    print(f"URLs Ã©chouÃ©es : {failed_urls} âŒ")
    print("\n--- âœ… Tous les scrapings sont terminÃ©s. ğŸ‰ ---")
